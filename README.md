# Edge Diffusion TTS

**Edge-optimized Diffusion Text-to-Speech with Progressive Distillation**

A lightweight diffusion-based TTS system optimized for edge device inference with **1-4 step generation**.

## Features

- ğŸš€ **Few-step inference**: Generate speech in 1-4 denoising steps (vs 1000 standard)
- ğŸ“± **Edge-optimized**: Depthwise separable convolutions, efficient attention
- ğŸ¯ **Progressive distillation**: Systematic step reduction 1000â†’4
- ğŸ”Š **HuBERT semantic encoding**: High-quality semantic representation
- âš¡ **Flash Attention**: Memory-efficient when available (PyTorch 2.0+)
- ğŸ **M1/M2 support**: Runs on Apple Silicon with MPS backend

## Project Structure

```
edge_diffusion_tts/
â”œâ”€â”€ __init__.py           # Package exports
â”œâ”€â”€ config.py             # Configuration dataclass
â”œâ”€â”€ schedule.py           # Diffusion schedules (DDPM/DDIM)
â”œâ”€â”€ inference.py          # Few-step inference engine
â”œâ”€â”€ train.py              # Main training loop
â”œâ”€â”€ cli.py                # CLI entry point
â”‚
â”œâ”€â”€ layers/               # Neural network layers
â”‚   â”œâ”€â”€ conv.py          # Depthwise separable convolutions
â”‚   â”œâ”€â”€ attention.py     # Efficient attention
â”‚   â”œâ”€â”€ transformer.py   # Transformer blocks
â”‚   â””â”€â”€ embeddings.py    # Time/position embeddings
â”‚
â”œâ”€â”€ models/               # Main models
â”‚   â”œâ”€â”€ vq.py            # Vector quantizer
â”‚   â”œâ”€â”€ encoder.py       # HuBERT semantic encoder
â”‚   â””â”€â”€ decoder.py       # Edge diffusion decoder
â”‚
â”œâ”€â”€ training/             # Training components
â”‚   â””â”€â”€ consistency.py   # Progressive/consistency distillation
â”‚
â”œâ”€â”€ data/                 # Data loading
â”‚   â”œâ”€â”€ dataset.py       # LJSpeech dataset
â”‚   â””â”€â”€ collate.py       # Batch collation
â”‚
â””â”€â”€ utils/                # Utilities
    â”œâ”€â”€ audio.py         # Mel normalization
    â”œâ”€â”€ visualization.py # Plotting & evaluation
    â””â”€â”€ export.py        # ONNX export
```

## Installation

```bash
# Using uv (recommended)
uv sync

# Or with pip
pip install -e .
```

## Quick Start

### Training

```bash
# Train with default settings
uv run train.py

# With custom settings
uv run train.py --device cuda --batch-size 8 --epochs 50

# Resume from checkpoint
uv run train.py --resume run_edge_diffusion/run_xxx/checkpoint_phase1.pt

# Export ONNX after training
uv run train.py --export
```

### Using the Package

```python
from edge_diffusion_tts import CFG, DiffusionSchedule, SemanticEncoder, EdgeDiffusionDecoder, EdgeInference

# Initialize
cfg = CFG()
schedule = DiffusionSchedule(cfg.diff_steps, device=cfg.device)
encoder = SemanticEncoder(cfg).to(cfg.device)
decoder = EdgeDiffusionDecoder(cfg).to(cfg.device)
inference = EdgeInference(cfg, schedule, encoder, decoder)

# Load trained weights
checkpoint = torch.load("edge_model_final.pt")
encoder.proj.load_state_dict(checkpoint["encoder_proj"])
encoder.vq.load_state_dict(checkpoint["encoder_vq"])
decoder.load_state_dict(checkpoint["decoder"])

# Generate in 4 steps!
mel = inference.generate_from_audio(waveform, num_steps=4)
```

## Training Phases

The training uses 3-phase progressive distillation:

### Phase 1: Standard Diffusion (30 epochs)
Train a full 1000-step diffusion model.

### Phase 2: Progressive Distillation (5 epochs per halving)
Progressively halve steps: 1000 â†’ 500 â†’ 250 â†’ 125 â†’ 64 â†’ 32 â†’ 16 â†’ 8 â†’ 4

### Phase 3: Consistency Distillation (10 epochs)
Fine-tune for 1-4 step generation with consistency loss.

## Configuration

Key hyperparameters in `CFG`:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `hidden` | 160 | Model hidden dimension (edge-optimized) |
| `layers` | 4 | Number of transformer layers |
| `heads` | 4 | Attention heads |
| `diff_steps` | 1000 | Total diffusion steps |
| `inference_steps` | 4 | Target inference steps |
| `batch_size` | 4 | Training batch size |
| `use_depthwise` | True | Use depthwise separable convs |

## Model Size

- **Decoder**: ~2.5 MB (FP32)
- **Semantic encoder (trainable)**: ~150KB
- **Total inference model**: ~3 MB

## Requirements

- Python 3.10+
- PyTorch 2.0+
- torchaudio
- transformers (for HuBERT)
- tensorboard, tqdm, matplotlib

## License

MIT
